\documentclass[aspectratio=169]{beamer}

% =========================
% Packages
% =========================
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{bm}
\usepackage{physics}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}

\usetheme{CambridgeUS}
\usecolortheme{default}

% =========================
% Metadata
% =========================
\title[Learning-Based Path Planning]{
Comparative Analysis of Data-Driven GNN-Based Path Planning\\
and Classical Search Algorithms (Grid Graphs, Pure A*, and GNN-Guided Search)
}
\author{
Author Name \\
Supervisor: Name
}
\institute{
Department of Computer Science \\
Institution Name
}
\date{\today}

% =========================
% Convenience macros
% =========================
\newcommand{\R}{\mathbb{R}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\bbone}{\mathbb{1}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newcommand{\softmax}{\operatorname{softmax}}
\newcommand{\sigmoid}{\operatorname{\sigma}}

%-------------------------------------------------
\begin{document}

%-------------------------------------------------
\begin{frame}
\titlepage
\end{frame}

%-------------------------------------------------
\begin{frame}{Outline}
\tableofcontents
\end{frame}

% =================================================
% 1) Intro + Motivation + Problem Definition
% =================================================
\section{Intro, Motivation, Problem Definition}

\begin{frame}{Motivation}{Why learn a planner if A* exists?}
\begin{itemize}
    \item \textbf{Classical planners} (BFS, Dijkstra, A*) are strong because they offer:
    \begin{itemize}
        \item \textbf{Completeness}: if a feasible path exists, search finds one.
        \item \textbf{Optimality}: shortest-path guarantees (e.g., A* with admissible heuristics).
        \item \textbf{Interpretability}: explicit cost function, transparent decisions.
    \end{itemize}
    \vspace{0.25cm}
    \item \textbf{But} classical search is \textbf{not amortised}: each new map requires exploring many states.
    \[
        \text{planning cost} \approx \text{node expansions + priority-queue ops}.
    \]
    \vspace{0.25cm}
    \item \textbf{Learning-based planning:} train a model to capture reusable structure across instances:
    \[
        f_\theta(G,s,g)\approx \pi^\star \quad \Rightarrow \quad \text{fast inference on unseen maps}.
    \]
    \item Practical goal in this project: compare \textbf{pure A*}, \textbf{greedy GNN decoding}, and a \textbf{hybrid GNN-guided A*}.
\end{itemize}
\end{frame}

\begin{frame}{Environment and Graph Definition}{What the notebook actually uses}
We consider an $N\times N$ grid world ($N=20$) with obstacles; free cells are nodes.
\[
G=(V,E), \quad |V| = N^2.
\]
4-neighborhood connectivity:
\[
(i,j)\sim (i\pm1,j),\ (i,j\pm1)\quad\Rightarrow\quad (u,v)\in E.
\]
Obstacles remove nodes/edges (edges only between free cells).

\vspace{0.2cm}
Planning objective (unit step costs in the notebook):
\[
C(\pi)=\sum_{t=1}^{T} 1 = T,\qquad
\pi^\star=\argmin_{\pi:s\to g} C(\pi).
\]

\vspace{0.2cm}
We report:
\[
\text{success rate},\quad \mathbb{E}[|\pi|],\quad \text{runtime},\quad \#\text{expansions}.
\]
\end{frame}

\begin{frame}{Supervised Learning Target}{Node labels used in the dataset}
Each training instance is generated by:
\[
(G,s,g)\xrightarrow{\text{A*}} \pi^\star.
\]
The notebook labels nodes with \textbf{soft supervision}:
\[
y_i=
\begin{cases}
1.0, & v_i\in \pi^\star\\
0.3, & \exists j\in\calN(i)\ \text{s.t.}\ v_j\in \pi^\star \ \text{(one-step near-path)}\\
0.0, & \text{otherwise.}
\end{cases}
\]

This reduces brittleness from:
\begin{itemize}
    \item multiple equivalent shortest paths (label ambiguity),
    \item near-miss nodes that are ``almost on-path''.
\end{itemize}
\end{frame}

% =================================================
% 2) Traditional Algorithms
% =================================================
\section{Traditional Algorithms (BFS/Dijkstra/A*)}

\begin{frame}{BFS (Unweighted)}{Baseline for shortest paths in hop count}
For unit edge costs, BFS computes hop distance:
\[
d(v)=\min_{\pi:s\to v}|\pi|.
\]
Guarantee:
\[
\text{BFS returns an optimal path if } w(e)=1\ \forall e.
\]
Complexity:
\[
\mathcal{O}(|V|+|E|).
\]
In grid graphs, BFS is a strong reference but still expands large regions when $g$ is far.
\end{frame}

\begin{frame}{Dijkstra (Weighted)}{General shortest paths}
Dijkstra maintains tentative distances:
\[
d(s)=0,\quad d(v)=\infty,
\]
and relaxes edges:
\[
d(v)\leftarrow \min\{d(v),\, d(u)+w(u,v)\}.
\]
Time (binary heap):
\[
\mathcal{O}(|E|\log|V|).
\]
In our notebook setting $w\equiv 1$, Dijkstra reduces to uniform-cost search (often similar to BFS behaviorally).
\end{frame}

\begin{frame}{A* (Heuristic Search)}{Pure A* baseline used for labels and evaluation}
A* ranks nodes by:
\[
f(v)=g(v)+h(v),
\]
where $g(v)$ is cost-to-come and $h(v)$ is heuristic to goal.

In the notebook:
\[
h(v)=\text{Manhattan}(v,g).
\]

Admissibility:
\[
h(v)\le h^\star(v)\ \Rightarrow\ \text{optimality}.
\]
A* efficiency is largely controlled by how informative $h$ is:
\[
\text{better } h \Rightarrow \text{fewer expansions (empirically)}.
\]
\end{frame}

% =================================================
% 3) Proposed Method: GNN Planner
% =================================================
\section{Proposed Method: GNN Planner}

\begin{frame}{Learning Formulation}{What the model predicts}
We learn a per-node predictor:
\[
\hat y_i \approx P(v_i \in \pi^\star \mid G,s,g).
\]
Define node features $\bm{x}_i\in\R^8$ (as in the notebook):
\[
\bm{x}_i =
\big[
\underbrace{\text{isBlocked}}_{1},
\underbrace{\text{isStart}}_{1},
\underbrace{\text{isGoal}}_{1},
\underbrace{x_{\text{norm}},y_{\text{norm}}}_{2},
\underbrace{d_{\text{Man}}(i,g)_{\text{norm}}, d_{\text{Man}}(i,s)_{\text{norm}}}_{2},
\underbrace{d_{\text{Euc}}(i,g)_{\text{norm}}}_{1}
\big].
\]
Training signal: soft labels $y_i\in\{0,0.3,1\}$ (converted to hard for metrics in evaluation).
\end{frame}

\begin{frame}{Message Passing}{GraphSAGE-style layer (used in code)}
The notebook model uses GraphSAGE convolution layers (SAGEConv).
Abstractly, with embeddings $\bm{h}^{(k)}_i\in\R^p$:
\[
\bm{h}_i^{(k+1)} =
\sigma\Big(
\bm{W}^{(k)}\bm{h}_i^{(k)}
\;+\;
\bm{U}^{(k)} \cdot
\textsc{Agg}\{ \bm{h}_j^{(k)}: j\in\calN(i)\}
\Big),
\]
where \textsc{Agg} is typically mean aggregation, and $\sigma(\cdot)$ is a nonlinearity.

\vspace{0.2cm}
In our implementation:
\begin{itemize}
    \item $L=5$ layers,
    \item hidden dim $p=128$,
    \item dropout $0.2$ after each layer,
    \item final linear head to a single logit per node.
\end{itemize}
\end{frame}

\begin{frame}{Output Head and Probabilities}{From embeddings to node probabilities}
Let the final embedding be $\bm{h}_i^{(L)}$. The model produces a logit:
\[
z_i = \bm{w}^\top \bm{h}_i^{(L)} + b,
\]
and probability:
\[
\hat y_i = \sigmoid(z_i)=\frac{1}{1+e^{-z_i}}.
\]

Two ways we use $\hat y$ in the notebook:
\begin{itemize}
    \item \textbf{Greedy decoding} (policy-like): always move to neighbor with maximum $\hat y$.
    \item \textbf{Hybrid search} (Neural A*): use $\hat y$ as a bias term within A* scoring.
\end{itemize}
\end{frame}

\begin{frame}{Decoding 1: Greedy Path Reconstruction}{Simple but fragile}
Given start $s$, goal $g$, and probabilities $\hat y$:
\[
v_{t+1} = \argmax_{u\in \calN(v_t)\setminus \text{Visited}} \hat y_u.
\]
Stop when reaching $g$ or no valid candidates.

\textbf{Why it can fail:}
\begin{itemize}
    \item local maxima: high-probability region that does not connect to goal,
    \item no backtracking: once a wrong step is taken, recovery is hard,
    \item graph connectivity constraints: predicted ``path mask'' may be disconnected.
\end{itemize}
Empirically, this shows up as low success rate despite good per-node metrics.
\end{frame}

\begin{frame}{Decoding 2: Neural A* (GNN-guided)}{Hybrid that preserves search structure}
The notebook defines a modified A* score:
\[
f(n) = g(n) + h(n) + \alpha\cdot (1-\hat y_n),
\]
where:
\begin{itemize}
    \item $g(n)$: cost-to-come (unit steps),
    \item $h(n)$: Manhattan heuristic,
    \item $\hat y_n$: GNN probability for node $n$,
    \item $\alpha>0$: strength of the learned bias.
\end{itemize}
Interpretation:
\[
(1-\hat y_n)\ \text{penalises low-probability nodes} \Rightarrow \text{search is nudged toward predicted path regions.}
\]
In the notebook, $\alpha=2.0$ is used.
\end{frame}

% =================================================
% 4) Implementation
% =================================================
\section{Implementation}

\begin{frame}{Data Generation Pipeline}{From random grids to PyG graphs}
Pipeline used:
\begin{enumerate}
    \item Sample grid with obstacle probability $p_{\text{block}}=0.2$.
    \item Sample start/goal from free cells.
    \item Run pure A* to get $\pi^\star$ (discard if no path).
    \item Convert grid to PyG graph:
    \[
    \text{Data}(x\in\R^{|V|\times 8},\ edge\_index\in\{1,\dots,|V|\}^{2\times |E|},\ y\in\R^{|V|}).
    \]
    \item Apply soft labels ($1.0/0.3/0.0$).
\end{enumerate}

Dataset size in the notebook run:
\[
|\calD|\approx 1186 \quad \text{(after discarding invalid samples)}.
\]
Split:
\[
70\%/15\%/15\% \Rightarrow (830,178,178).
\]
\end{frame}

\begin{frame}{Training Setup}{Loss, imbalance handling, optimiser}
Model: GraphSAGE, $L=5$, hidden $128$, dropout $0.2$.

\vspace{0.2cm}
Loss: BCE with logits, computed \textbf{only on free cells}:
\[
\calL =
-\sum_{i\in \text{Free}} \Big(
y_i\log \sigmoid(z_i) + (1-y_i)\log(1-\sigmoid(z_i))
\Big).
\]

Class imbalance is handled via \textbf{pos\_weight}:
\[
\calL_{\text{pw}} = \text{BCEWithLogitsLoss}(\text{pos\_weight}),
\qquad
\text{pos\_weight} \approx \frac{\#\text{neg}}{\#\text{pos}} \approx 17.995.
\]

Optimiser:
\[
\theta \leftarrow \theta - \eta \nabla_\theta \calL,\quad \eta=10^{-3} \ \ (\text{Adam}).
\]
\end{frame}

\begin{frame}{Evaluation Protocol}{Thresholding, metrics, and why F1 matters}
Probabilities are thresholded:
\[
\hat y_i^{(\text{hard})} = \bbone[\sigmoid(z_i)>\tau].
\]
Soft labels are converted to hard for metrics:
\[
y_i^{(\text{hard})}=\bbone[y_i\ge 0.5].
\]

We report:
\[
\text{Accuracy}=\frac{TP+TN}{TP+TN+FP+FN},
\quad
\text{Precision}=\frac{TP}{TP+FP},
\quad
\text{Recall}=\frac{TP}{TP+FN},
\]
\[
F_1=\frac{2\cdot \text{Prec}\cdot \text{Rec}}{\text{Prec}+\text{Rec}}.
\]

Because positives are sparse (path nodes), \textbf{accuracy can look high even when path quality is poor}; hence we tune $\tau$ on validation to maximise $F_1$.
\end{frame}

% =================================================
% 5) Experiments & Results
% =================================================
\section{Experiments \& Results}

\begin{frame}{Threshold Tuning Result}{Validation F1 maximisation}
Validation sweep chooses:
\[
\tau^\star = \argmax_{\tau\in[0,1]} F_1(\tau).
\]
In the notebook:
\[
\tau^\star \approx 0.85,\qquad F_1^{(\text{val})}\approx 0.427.
\]

Interpretation:
\begin{itemize}
    \item a relatively high threshold reduces false positives (important in sparse labels),
    \item but may miss true path nodes if the model is under-confident.
\end{itemize}
\end{frame}

\begin{frame}{Test Classification Metrics (Node-Level)}{As reported in the notebook}
With $\tau^\star=0.85$, test performance:
\[
\text{Loss}\approx 0.7233,\quad
\text{Acc}\approx 0.9277,\quad
\text{Prec}\approx 0.3493,\quad
\text{Rec}\approx 0.5779,\quad
F_1\approx 0.4355.
\]

\vspace{0.2cm}
Reading this correctly:
\begin{itemize}
    \item accuracy is inflated by many negatives (non-path nodes),
    \item precision/recall/F1 better reflect detection of the path structure,
    \item node-level metrics do \textbf{not} automatically imply successful path reconstruction.
\end{itemize}
\end{frame}

\begin{frame}{Planner-Level Evaluation}{Pure A* vs Greedy-GNN vs Neural A*}
Planners evaluated on the same test graphs:
\begin{itemize}
    \item \textbf{Pure A*}: baseline, Manhattan heuristic.
    \item \textbf{Greedy-GNN}: follow max-prob neighbor.
    \item \textbf{Neural A*}: A* with bias $\alpha(1-\hat y)$, $\alpha=2.0$.
\end{itemize}

Reported outcomes:
\begin{align*}
\text{success: } & \text{A*}=1.0,\ \text{Greedy}\approx 0.348,\ \text{Neural A*}=1.0,\\
\mathbb{E}[|\pi|]:\ & \text{A*}\approx 14.43,\ \text{Greedy}\approx 17.98,\ \text{Neural A*}\approx 14.43.
\end{align*}

Key point: \textbf{GNN-only greedy decoding fails often}, but \textbf{hybrid search retains success and optimal-length behavior in this setting}.
\end{frame}

\begin{frame}{Runtime and Expansions}{Why expansions matter more than wall-clock}
Average times (seconds, as measured):
\[
t_{\text{A*}}\approx 4.56\!\times\!10^{-4},\quad
t_{\text{Greedy}}\approx 8.34\!\times\!10^{-4},\quad
t_{\text{Neural}}\approx 9.45\!\times\!10^{-4}.
\]

A more stable measure is \textbf{node expansions} (nodes popped from the heap):
\[
\text{A* expansions}\approx 29.81,\qquad
\text{Neural A* expansions}\approx 29.30.
\]

Interpretation:
\begin{itemize}
    \item In this run, Neural A* matches A* closely (success and length), with similar expansions.
    \item Runtime includes overheads (Python, GPU transfer, etc.), so expansions is a cleaner algorithmic signal.
\end{itemize}
\end{frame}

\begin{frame}{Ablations to Add (Recommended)}{To make results scientifically stronger}
Ablations that map directly to your code:
\begin{itemize}
    \item \textbf{Heuristic strength:} sweep $\alpha$ in Neural A*:
    \[
    f = g + h + \alpha(1-\hat y) \quad \Rightarrow \quad \alpha \in \{0,0.5,1,2,5\}.
    \]
    \item \textbf{Depth:} vary $L$ (e.g., $2,3,5,8$) to test over-smoothing vs expressivity.
    \item \textbf{Labeling:} compare hard labels ($\{0,1\}$) vs soft labels ($\{0,0.3,1\}$).
    \item \textbf{Features:} remove Manhattan-to-goal / Euclid-to-goal to measure reliance on engineered signals.
\end{itemize}
\end{frame}

% =================================================
% 6) Limitations
% =================================================
\section{Limitations}

\begin{frame}{Limitations}{What the current notebook does \emph{not} guarantee}
\begin{itemize}
    \item \textbf{Node classification $\neq$ path correctness:} high node-level accuracy does not ensure a connected, feasible path.
    \item \textbf{Greedy decoding brittleness:} success rate $\approx 0.35$ despite decent F1.
    \item \textbf{No optimality guarantee for learned bias:} Neural A* modifies $f$; admissibility is not preserved in general:
    \[
    h'(n)=h(n)+\alpha(1-\hat y_n)\ \not\le\ h^\star(n)\ \text{necessarily}.
    \]
    \item \textbf{Distribution dependence:} trained on $N=20$, $p_{\text{block}}=0.2$; performance may shift for different sizes/densities.
\end{itemize}
\end{frame}

% =================================================
% 7) Conclusion
% =================================================
\section{Conclusion}

\begin{frame}{Conclusion}{What we can honestly claim from current results}
\begin{itemize}
    \item We built a full pipeline: grid $\rightarrow$ A* labels $\rightarrow$ PyG graphs $\rightarrow$ GraphSAGE training $\rightarrow$ planner evaluation.
    \item The model learns useful \textbf{local path saliency}: test $F_1 \approx 0.436$ under tuned thresholding.
    \item \textbf{Greedy GNN planning is unreliable} on this setup (success $\approx 0.35$).
    \item \textbf{Hybrid search (Neural A*) recovers robust planning:} success $=1.0$ and average length matches A* in this run.
\end{itemize}
\end{frame}

% =================================================
% 8) Future Work
% =================================================
\section{Future Work}

\begin{frame}{Future Work}{Concrete next steps that align with your code}
\begin{itemize}
    \item \textbf{Generalisation tests:} train on $N=20$ and test on $N\in\{25,30,40\}$; also sweep $p_{\text{block}}$.
    \item \textbf{Hybrid improvements:} learn a value function $\hat V_\theta(n)\approx h^\star(n)$ and use:
    \[
    f(n)=g(n)+\hat V_\theta(n),
    \]
    (optionally enforce calibration / monotonicity).
    \item \textbf{Better decoding:} replace greedy with constrained shortest-path on high-prob subgraph, or beam search.
    \item \textbf{Report expansions formally:} expansions vs. graph size is the cleanest algorithmic scaling metric.
\end{itemize}
\end{frame}

% =================================================
% 9) Related Work / Literature
% =================================================
\section{Related Work / Literature}

\begin{frame}{Related Work (Pointers)}{Where this sits in the literature}
\begin{itemize}
    \item \textbf{Classical search:} shortest path and heuristic search (Dijkstra, A*).
    \item \textbf{GNN foundations:} message passing / inductive learning on graphs.
    \item \textbf{Learning to plan:} learning heuristics, value functions, or policies to guide search.
\end{itemize}

\vspace{0.2cm}
For your final version, include 5--10 key citations (A*, Dijkstra, GraphSAGE, MPNNs, learning-to-search).
\end{frame}

\begin{frame}{What is the “contribution” here?}{How to phrase it academically}
A defensible scientific framing:
\begin{itemize}
    \item A clean \textbf{end-to-end experimental pipeline} for comparing:
    \[
    \text{Pure A*} \;\; \text{vs.} \;\; \text{Greedy learned decoding} \;\; \text{vs.} \;\; \text{Hybrid learned-guided A*}.
    \]
    \item Empirical evidence that \textbf{node-level prediction quality does not imply planner success} (greedy failure).
    \item Empirical evidence that \textbf{hybridisation preserves classical reliability} while injecting learned bias.
\end{itemize}
This is strong as an MSc project if you back it with generalisation + ablations.
\end{frame}

% =================================================
% 10) Q&A
% =================================================
\section*{Q\&A}

\begin{frame}{Q\&A}{}
\centering
{\Large Questions?}

\vspace{0.4cm}
\small
(Optionally: add preprint / repo link in footer)
\end{frame}

\end{document}
